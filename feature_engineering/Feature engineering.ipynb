{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Basic Siamese RNN (LSTM)\n",
    "   - Use Jieba tokenizer to get tokens\n",
    "   - Build dictionary\n",
    "   - Turn titles into index vectors\n",
    "   - Zero padding to make fixed-length index vector \n",
    "   - Turn label into one-hot vectors\n",
    "   - Siamese LSTM model \n",
    "   - Train and test\n",
    "   - Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "import os\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_PATH = './project1_data/train.csv'\n",
    "TEST_CSV_PATH = './project1_data/test.csv'\n",
    "TOKENIZED_TRAIN_CSV_PATH = './project1_data/tokenized_train.csv'\n",
    "TOKENIZED_TEST_CSV_PATH = './project1_data/tokenized_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>There are two new old-age insurance benefits f...</td>\n",
       "      <td>Police disprove \"bird's nest congress each per...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n",
       "      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tid1  tid2                          title1_zh                  title2_zh  \\\n",
       "id                                                                             \n",
       "0      0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗   警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n",
       "3      2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n",
       "1      2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港       GDP首超香港？深圳澄清：还差一点点……   \n",
       "\n",
       "                                            title1_en  \\\n",
       "id                                                      \n",
       "0   There are two new old-age insurance benefits f...   \n",
       "3   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "1   \"If you do not come to Shenzhen, sooner or lat...   \n",
       "\n",
       "                                            title2_en      label  \n",
       "id                                                                \n",
       "0   Police disprove \"bird's nest congress each per...  unrelated  \n",
       "3   Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated  \n",
       "1   The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_CSV_PATH, index_col='id')\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Jieba tokenizer to get tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jieba_tokenizer(text):\n",
    "    words = pseg.cut(text)\n",
    "    return ' '.join([word for word, flag in words if flag != 'x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title1_zh    False\n",
       "title2_zh     True\n",
       "label        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title1_zh    False\n",
       "title2_zh    False\n",
       "label        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.title2_zh.fillna('UNKNOWN', inplace=True)\n",
    "train.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:7: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:7: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<ipython-input-9-2085024a8821>:7: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert((data.index == res.index).all(), 'Something error when merge data')\n"
     ]
    }
   ],
   "source": [
    "def process(data):\n",
    "    res = data.apply(jieba_tokenizer)\n",
    "    return res\n",
    "\n",
    "def check_merge_idx(data, res):\n",
    "    assert((data.index == res.index).all(), 'Something error when merge data')\n",
    "\n",
    "def parallelize(data, func):\n",
    "    from multiprocessing import cpu_count, Pool\n",
    "    cores = partitions = cpu_count()\n",
    "    data_split = np.array_split(data, partitions)\n",
    "    pool = Pool(cores)\n",
    "    res = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    check_merge_idx(data, res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(train.index == train.title1_zh.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Loading model cost 1.406 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.420 seconds.\n",
      "Loading model cost 1.390 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.368 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Loading model cost 1.822 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.794 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.828 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.865 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(TOKENIZED_TRAIN_CSV_PATH):\n",
    "    print(\"Use prepared tokenized train data\")\n",
    "    train = pd.read_csv(TOKENIZED_TRAIN_CSV_PATH, index_col='id')\n",
    "else:\n",
    "    print(\"start to training\")\n",
    "    train['title1_tokenized'] = parallelize(train.loc[:, 'title1_zh'], process)\n",
    "    train['title2_tokenized'] = parallelize(train.loc[:, 'title2_zh'], process)\n",
    "    train.to_csv('tokenized_train.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title1_tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n",
       "      <td>2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"用大蒜鉴别地沟油的方法,怎么鉴别地沟油</td>\n",
       "      <td>用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title1_zh  \\\n",
       "id                                      \n",
       "0       2017养老保险又新增两项，农村老人人人可申领，你领到了吗   \n",
       "3   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "1   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "2   \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港   \n",
       "9                \"用大蒜鉴别地沟油的方法,怎么鉴别地沟油   \n",
       "\n",
       "                                   title1_tokenized  \n",
       "id                                                   \n",
       "0          2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗  \n",
       "3   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港  \n",
       "1   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港  \n",
       "2   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港  \n",
       "9                        用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[:, [\"title1_zh\", \"title1_tokenized\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title2_tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>GDP 首 超 香港 深圳 澄清 还 差 一点点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>去年 深圳 GDP 首 超 香港 深圳 统计局 辟谣 还 差 611 亿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n",
       "      <td>吃 了 30 年 食用油 才 知道 一片 大蒜 轻松 鉴别 地沟油</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title2_zh                      title2_tokenized\n",
       "id                                                                  \n",
       "0     警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京     警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京\n",
       "3    深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小    深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小\n",
       "1         GDP首超香港？深圳澄清：还差一点点……              GDP 首 超 香港 深圳 澄清 还 差 一点点\n",
       "2   去年深圳GDP首超香港？深圳统计局辟谣：还差611亿  去年 深圳 GDP 首 超 香港 深圳 统计局 辟谣 还 差 611 亿\n",
       "9      吃了30年食用油才知道，一片大蒜轻松鉴别地沟油     吃 了 30 年 食用油 才 知道 一片 大蒜 轻松 鉴别 地沟油"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[:, [\"title2_zh\", \"title2_tokenized\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna('UNKNOWN', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 70000\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(641104,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_x1 = train.title1_tokenized\n",
    "corpus_x2 = train.title2_tokenized\n",
    "corpus = pd.concat([corpus_x1, corpus_x2])\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              title\n",
       "id                                                 \n",
       "0          2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗\n",
       "3   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
       "1   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
       "2   你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
       "9                        用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(corpus.iloc[:5],\n",
    "             columns=['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./project1_data/corpus.txt', 'w', encoding='utf-8')as f:\n",
    "    for sent in corpus:\n",
    "        f.write(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn titles into index vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(corpus)\n",
    "x1_train = tokenizer.texts_to_sequences(corpus_x1)\n",
    "x2_train = tokenizer.texts_to_sequences(corpus_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero padding to make fixed-length index vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 20\n",
    "x1_train = keras.preprocessing.sequence.pad_sequences(x1_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "x2_train = keras.preprocessing.sequence.pad_sequences(x2_train, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn labels into one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "\n",
    "label_to_index = {\n",
    "    'unrelated': 0, \n",
    "    'agreed': 1, \n",
    "    'disagreed': 2\n",
    "}\n",
    "\n",
    "\n",
    "y_train = train.label.apply(\n",
    "    lambda x: label_to_index[x])\n",
    "\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320552,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese LSTM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "VALIDATION_RATIO = 0.1\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "x1_train, x1_val, x2_train, x2_val, y_train, y_val = \\\n",
    "    train_test_split(\n",
    "        x1_train, x2_train, y_train, \n",
    "        test_size=VALIDATION_RATIO, \n",
    "        random_state=RANDOM_STATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "NUM_CLASSES = 3\n",
    "MAX_NUM_WORDS = 70000\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "NUM_EMBEDDING_DIM = 256\n",
    "NUM_LSTM_UNITS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 20, 256)      17920000    input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 128)          197120      embedding_2[0][0]                \n",
      "                                                                 embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           lstm_2[0][0]                     \n",
      "                                                                 lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            771         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 18,117,891\n",
      "Trainable params: 18,117,891\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Input\n",
    "from keras.layers import Embedding,LSTM, concatenate, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "top_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n",
    "bm_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "top_embedded = embedding_layer(top_input)\n",
    "bm_embedded = embedding_layer(bm_input)\n",
    "\n",
    "shared_lstm = LSTM(NUM_LSTM_UNITS)\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "merged = concatenate([top_output, bm_output], axis=-1)\n",
    "\n",
    "dense =  Dense(units=NUM_CLASSES, activation='softmax')\n",
    "predictions = dense(merged)\n",
    "\n",
    "\n",
    "model = Model(inputs=[top_input, bm_input], outputs=predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "opt = Adam(lr=lr, decay=lr/50)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288496, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    x=[x1_train, x2_train], \n",
    "    y=y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=(\n",
    "        [x1_val, x2_val], \n",
    "        y_val\n",
    "    ),\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use raw test csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "2019-05-10 11:04:25,240 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "2019-05-10 11:04:25,248 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "2019-05-10 11:04:25,265 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "2019-05-10 11:04:25,310 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Dumping model to file cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Dumping model to file cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Dumping model to file cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "2019-05-10 11:04:27,177 : DEBUG : Dumping model to file cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "2019-05-10 11:04:27,178 : DEBUG : Dumping model to file cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "2019-05-10 11:04:27,177 : DEBUG : Dumping model to file cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "2019-05-10 11:04:27,177 : DEBUG : Dumping model to file cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Loading model cost 2.000 seconds.\n",
      "2019-05-10 11:04:27,316 : DEBUG : Loading model cost 2.000 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-05-10 11:04:27,323 : DEBUG : Prefix dict has been built succesfully.\n",
      "Loading model cost 2.071 seconds.\n",
      "Loading model cost 1.975 seconds.\n",
      "2019-05-10 11:04:27,331 : DEBUG : Loading model cost 1.975 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-05-10 11:04:27,329 : DEBUG : Loading model cost 2.071 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-05-10 11:04:27,340 : DEBUG : Prefix dict has been built succesfully.\n",
      "2019-05-10 11:04:27,346 : DEBUG : Prefix dict has been built succesfully.\n",
      "Loading model cost 2.096 seconds.\n",
      "2019-05-10 11:04:27,353 : DEBUG : Loading model cost 2.096 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-05-10 11:04:27,367 : DEBUG : Prefix dict has been built succesfully.\n",
      "Building prefix dict from the default dictionary ...\n",
      "2019-05-10 11:05:30,551 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "2019-05-10 11:05:30,557 : DEBUG : Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2019-05-10 11:05:30,582 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "2019-05-10 11:05:30,588 : DEBUG : Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2019-05-10 11:05:30,630 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "2019-05-10 11:05:30,637 : DEBUG : Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "2019-05-10 11:05:30,695 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "2019-05-10 11:05:30,701 : DEBUG : Loading model from cache /var/folders/90/jkdn_401557c3mztpykpjdfw0000gn/T/jieba.cache\n",
      "Loading model cost 1.836 seconds.\n",
      "Loading model cost 1.756 seconds.\n",
      "2019-05-10 11:05:32,393 : DEBUG : Loading model cost 1.756 seconds.\n",
      "2019-05-10 11:05:32,393 : DEBUG : Loading model cost 1.836 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-05-10 11:05:32,398 : DEBUG : Prefix dict has been built succesfully.\n",
      "2019-05-10 11:05:32,398 : DEBUG : Prefix dict has been built succesfully.\n",
      "Loading model cost 1.720 seconds.\n",
      "2019-05-10 11:05:32,421 : DEBUG : Loading model cost 1.720 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "Loading model cost 1.846 seconds.\n",
      "2019-05-10 11:05:32,430 : DEBUG : Prefix dict has been built succesfully.\n",
      "2019-05-10 11:05:32,433 : DEBUG : Loading model cost 1.846 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-05-10 11:05:32,445 : DEBUG : Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>title1_zh</th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>title1_tokenized</th>\n",
       "      <th>title2_tokenized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321187</th>\n",
       "      <td>167562</td>\n",
       "      <td>59521</td>\n",
       "      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n",
       "      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n",
       "      <td>egypt 's presidential election failed to win m...</td>\n",
       "      <td>Lyon! Lyon officials have denied that Felipe F...</td>\n",
       "      <td>萨拉 赫 人气 爆棚 埃及 总统大选 未 参选 获 百万 选票 现任 总统 压力 山 大</td>\n",
       "      <td>辟谣 里昂 官方 否认 费 基尔 加盟 利物浦 难道 是 价格 没 谈拢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321190</th>\n",
       "      <td>167564</td>\n",
       "      <td>91315</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>The Top 10 Americans believe that the Lizard M...</td>\n",
       "      <td>萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思</td>\n",
       "      <td>10 大 最 让 美国 人 相信 的 荒诞 谣言 如 蜥蜴人 掌控 着 美国</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321189</th>\n",
       "      <td>167563</td>\n",
       "      <td>167564</td>\n",
       "      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n",
       "      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n",
       "      <td>Will the United States wage war on Iraq withou...</td>\n",
       "      <td>A message from Saddam Hussein after he was cap...</td>\n",
       "      <td>萨达姆 此项 计划 没有 此国 破坏 的话 美国 还 会 对 伊拉克 发动战争 吗</td>\n",
       "      <td>萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          tid1    tid2                        title1_zh  \\\n",
       "id                                                        \n",
       "321187  167562   59521  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大   \n",
       "321190  167564   91315              萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "321189  167563  167564    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗   \n",
       "\n",
       "                          title2_zh  \\\n",
       "id                                    \n",
       "321187  辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？   \n",
       "321190    10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国   \n",
       "321189          萨达姆被捕后告诫美国的一句话，发人深思   \n",
       "\n",
       "                                                title1_en  \\\n",
       "id                                                          \n",
       "321187  egypt 's presidential election failed to win m...   \n",
       "321190  A message from Saddam Hussein after he was cap...   \n",
       "321189  Will the United States wage war on Iraq withou...   \n",
       "\n",
       "                                                title2_en  \\\n",
       "id                                                          \n",
       "321187  Lyon! Lyon officials have denied that Felipe F...   \n",
       "321190  The Top 10 Americans believe that the Lizard M...   \n",
       "321189  A message from Saddam Hussein after he was cap...   \n",
       "\n",
       "                                    title1_tokenized  \\\n",
       "id                                                     \n",
       "321187  萨拉 赫 人气 爆棚 埃及 总统大选 未 参选 获 百万 选票 现任 总统 压力 山 大   \n",
       "321190                    萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思   \n",
       "321189     萨达姆 此项 计划 没有 此国 破坏 的话 美国 还 会 对 伊拉克 发动战争 吗   \n",
       "\n",
       "                              title2_tokenized  \n",
       "id                                              \n",
       "321187    辟谣 里昂 官方 否认 费 基尔 加盟 利物浦 难道 是 价格 没 谈拢  \n",
       "321190  10 大 最 让 美国 人 相信 的 荒诞 谣言 如 蜥蜴人 掌控 着 美国  \n",
       "321189              萨达姆 被捕 后 告诫 美国 的 一句 话 发人深思  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized\n",
    "import pandas as pd\n",
    "if os.path.exists(TOKENIZED_TEST_CSV_PATH):\n",
    "    print(\"Use tokenized test csv\")\n",
    "    test = pd.read_csv(TOKENIZED_TEST_CSV_PATH, index_col=0)\n",
    "else:\n",
    "    print(\"Use raw test csv\")\n",
    "    test = pd.read_csv(TEST_CSV_PATH, index_col=0)\n",
    "    test.fillna('UNKNOWN', inplace=True)\n",
    "    test['title1_tokenized'] = parallelize(test.loc[:, 'title1_zh'], process)\n",
    "    test['title2_tokenized'] = parallelize(test.loc[:, 'title2_zh'], process)\n",
    "    test.fillna('UNKNOWN', inplace=True)\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index vectors\n",
    "x1_test = tokenizer.texts_to_sequences(test.title1_tokenized)\n",
    "x2_test = tokenizer.texts_to_sequences(test.title2_tokenized)\n",
    "\n",
    "# zero padding\n",
    "x1_test = keras.preprocessing.sequence.pad_sequences(x1_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "x2_test = keras.preprocessing.sequence.pad_sequences(x2_test,maxlen=MAX_SEQUENCE_LENGTH)    \n",
    "\n",
    "# predict \n",
    "predictions = model.predict([x1_test, x2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "test['Category'] = [index_to_label[idx] for idx in np.argmax(predictions, axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test.loc[:, ['Category']].reset_index()\n",
    "submission.columns = ['Id', 'Category']\n",
    "submission.to_csv('./submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-trained embedding\n",
    "- word2vec\n",
    "- doc2vec\n",
    "- fastText\n",
    "- bert-as-service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec (word-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 11:18:04,636 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-05-10 11:18:04,638 : INFO : collecting all words and their counts\n",
      "2019-05-10 11:18:04,639 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:18:05,991 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-10 11:18:07,474 : INFO : collected 298614 word types from a corpus of 7483099 raw words and 749 sentences\n",
      "2019-05-10 11:18:07,475 : INFO : Loading a fresh vocabulary\n",
      "2019-05-10 11:18:07,669 : INFO : effective_min_count=5 retains 63882 unique words (21% of original 298614, drops 234732)\n",
      "2019-05-10 11:18:07,670 : INFO : effective_min_count=5 leaves 7150609 word corpus (95% of original 7483099, drops 332490)\n",
      "2019-05-10 11:18:07,913 : INFO : deleting the raw counts dictionary of 298614 items\n",
      "2019-05-10 11:18:07,962 : INFO : sample=0.001 downsamples 27 most-common words\n",
      "2019-05-10 11:18:07,963 : INFO : downsampling leaves estimated 6665665 word corpus (93.2% of prior 7150609)\n",
      "2019-05-10 11:18:08,147 : INFO : estimated required memory for 63882 words and 250 dimensions: 159705000 bytes\n",
      "2019-05-10 11:18:08,148 : INFO : resetting layer weights\n",
      "2019-05-10 11:18:09,029 : INFO : training model with 3 workers on 63882 vocabulary and 250 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-10 11:18:09,033 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:18:10,036 : INFO : EPOCH 1 - PROGRESS: at 3.74% examples, 258513 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:11,046 : INFO : EPOCH 1 - PROGRESS: at 18.42% examples, 628341 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:12,063 : INFO : EPOCH 1 - PROGRESS: at 30.44% examples, 688647 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:13,077 : INFO : EPOCH 1 - PROGRESS: at 44.59% examples, 754782 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:14,088 : INFO : EPOCH 1 - PROGRESS: at 58.88% examples, 791694 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:15,089 : INFO : EPOCH 1 - PROGRESS: at 72.76% examples, 809442 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:16,104 : INFO : EPOCH 1 - PROGRESS: at 85.58% examples, 811056 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-10 11:18:17,219 : INFO : EPOCH 1 - PROGRESS: at 98.80% examples, 805327 words/s, in_qsize 5, out_qsize 1\n",
      "2019-05-10 11:18:17,293 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:18:17,294 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:18:17,305 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:18:17,305 : INFO : EPOCH - 1 : training on 7483099 raw words (6664713 effective words) took 8.3s, 805752 effective words/s\n",
      "2019-05-10 11:18:17,309 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:18:18,317 : INFO : EPOCH 2 - PROGRESS: at 4.27% examples, 293004 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:19,321 : INFO : EPOCH 2 - PROGRESS: at 16.69% examples, 569625 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:20,335 : INFO : EPOCH 2 - PROGRESS: at 30.84% examples, 698499 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:21,343 : INFO : EPOCH 2 - PROGRESS: at 44.86% examples, 761096 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:22,353 : INFO : EPOCH 2 - PROGRESS: at 58.21% examples, 784791 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:23,355 : INFO : EPOCH 2 - PROGRESS: at 71.83% examples, 800795 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:24,368 : INFO : EPOCH 2 - PROGRESS: at 83.98% examples, 797507 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:25,379 : INFO : EPOCH 2 - PROGRESS: at 96.53% examples, 798579 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:25,757 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:18:25,760 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:18:25,766 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:18:25,767 : INFO : EPOCH - 2 : training on 7483099 raw words (6665013 effective words) took 8.5s, 788108 effective words/s\n",
      "2019-05-10 11:18:25,769 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:18:26,783 : INFO : EPOCH 3 - PROGRESS: at 3.74% examples, 255690 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:27,790 : INFO : EPOCH 3 - PROGRESS: at 17.09% examples, 580836 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:28,809 : INFO : EPOCH 3 - PROGRESS: at 28.44% examples, 641632 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:29,816 : INFO : EPOCH 3 - PROGRESS: at 41.79% examples, 707056 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-10 11:18:30,828 : INFO : EPOCH 3 - PROGRESS: at 55.01% examples, 741411 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:31,836 : INFO : EPOCH 3 - PROGRESS: at 67.69% examples, 754083 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:32,843 : INFO : EPOCH 3 - PROGRESS: at 81.58% examples, 773792 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:33,845 : INFO : EPOCH 3 - PROGRESS: at 93.99% examples, 777841 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:34,400 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:18:34,405 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:18:34,420 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:18:34,421 : INFO : EPOCH - 3 : training on 7483099 raw words (6665976 effective words) took 8.7s, 770445 effective words/s\n",
      "2019-05-10 11:18:34,424 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:18:35,426 : INFO : EPOCH 4 - PROGRESS: at 4.54% examples, 313286 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:36,429 : INFO : EPOCH 4 - PROGRESS: at 19.23% examples, 658105 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:37,431 : INFO : EPOCH 4 - PROGRESS: at 33.91% examples, 772394 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:38,435 : INFO : EPOCH 4 - PROGRESS: at 48.87% examples, 834647 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:39,454 : INFO : EPOCH 4 - PROGRESS: at 60.35% examples, 814606 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:40,457 : INFO : EPOCH 4 - PROGRESS: at 69.96% examples, 782419 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-10 11:18:41,470 : INFO : EPOCH 4 - PROGRESS: at 82.91% examples, 789239 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-10 11:18:42,480 : INFO : EPOCH 4 - PROGRESS: at 92.52% examples, 767812 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:43,131 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:18:43,137 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:18:43,150 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:18:43,151 : INFO : EPOCH - 4 : training on 7483099 raw words (6665028 effective words) took 8.7s, 763798 effective words/s\n",
      "2019-05-10 11:18:43,155 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:18:44,157 : INFO : EPOCH 5 - PROGRESS: at 4.94% examples, 340849 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:45,172 : INFO : EPOCH 5 - PROGRESS: at 19.49% examples, 663282 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:46,180 : INFO : EPOCH 5 - PROGRESS: at 34.18% examples, 773970 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:47,185 : INFO : EPOCH 5 - PROGRESS: at 49.00% examples, 833118 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:48,188 : INFO : EPOCH 5 - PROGRESS: at 63.02% examples, 848606 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:49,194 : INFO : EPOCH 5 - PROGRESS: at 76.64% examples, 853479 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:50,198 : INFO : EPOCH 5 - PROGRESS: at 90.52% examples, 859799 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:18:50,972 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:18:50,980 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:18:50,991 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:18:50,992 : INFO : EPOCH - 5 : training on 7483099 raw words (6666001 effective words) took 7.8s, 850607 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 11:18:50,993 : INFO : training on a 37415495 raw words (33326731 effective words) took 42.0s, 794212 effective words/s\n",
      "2019-05-10 11:18:51,041 : INFO : saving Word2Vec object under word2vec250_word.model, separately None\n",
      "2019-05-10 11:18:51,043 : INFO : storing np array 'vectors' to word2vec250_word.model.wv.vectors.npy\n",
      "2019-05-10 11:18:51,239 : INFO : not storing attribute vectors_norm\n",
      "2019-05-10 11:18:51,240 : INFO : storing np array 'syn1neg' to word2vec250_word.model.trainables.syn1neg.npy\n",
      "2019-05-10 11:18:51,431 : INFO : not storing attribute cum_table\n",
      "2019-05-10 11:18:51,432 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:18:51,557 : INFO : saved word2vec250_word.model\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gensim.models import word2vec\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sentences = word2vec.LineSentence('./project1_data/corpus.txt')\n",
    "w2v_model = word2vec.Word2Vec(sentences, size=250, workers=3)\n",
    "w2v_model.save(\"word2vec250_word.model\")\n",
    "# how to load model\n",
    "#w2v = word2vec.Word2Vec.load(\"word2vec250_word.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec (char-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = np.unique([v for v in np.concatenate([train.title1_zh.unique(), train.title2_zh.unique()]) if type(v) == str])\n",
    "test_corpus = np.unique([v for v in np.concatenate([test.title1_zh.unique(), test.title2_zh.unique()]) if type(v) == str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_corpus_char = np.concatenate([train_corpus, test_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./project1_data/corpus_char.txt', 'w', encoding='utf-8') as corpus:\n",
    "    for sentence in all_corpus_char:\n",
    "        for char in sentence:\n",
    "            corpus.write(char + ' ')\n",
    "        corpus.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 11:25:39,586 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-05-10 11:25:39,587 : INFO : collecting all words and their counts\n",
      "2019-05-10 11:25:39,588 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:25:39,590 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-10 11:25:39,672 : INFO : PROGRESS: at sentence #10000, processed 268913 words, keeping 3122 word types\n",
      "2019-05-10 11:25:39,744 : INFO : PROGRESS: at sentence #20000, processed 527364 words, keeping 3764 word types\n",
      "2019-05-10 11:25:39,813 : INFO : PROGRESS: at sentence #30000, processed 776712 words, keeping 4033 word types\n",
      "2019-05-10 11:25:39,881 : INFO : PROGRESS: at sentence #40000, processed 1026225 words, keeping 4173 word types\n",
      "2019-05-10 11:25:39,949 : INFO : PROGRESS: at sentence #50000, processed 1274541 words, keeping 4291 word types\n",
      "2019-05-10 11:25:40,018 : INFO : PROGRESS: at sentence #60000, processed 1509680 words, keeping 4441 word types\n",
      "2019-05-10 11:25:40,086 : INFO : PROGRESS: at sentence #70000, processed 1758381 words, keeping 4532 word types\n",
      "2019-05-10 11:25:40,153 : INFO : PROGRESS: at sentence #80000, processed 2002014 words, keeping 4639 word types\n",
      "2019-05-10 11:25:40,220 : INFO : PROGRESS: at sentence #90000, processed 2243756 words, keeping 4717 word types\n",
      "2019-05-10 11:25:40,288 : INFO : PROGRESS: at sentence #100000, processed 2490941 words, keeping 4807 word types\n",
      "2019-05-10 11:25:40,354 : INFO : PROGRESS: at sentence #110000, processed 2731969 words, keeping 4863 word types\n",
      "2019-05-10 11:25:40,422 : INFO : PROGRESS: at sentence #120000, processed 2977005 words, keeping 4912 word types\n",
      "2019-05-10 11:25:40,490 : INFO : PROGRESS: at sentence #130000, processed 3219043 words, keeping 4977 word types\n",
      "2019-05-10 11:25:40,559 : INFO : PROGRESS: at sentence #140000, processed 3467494 words, keeping 5026 word types\n",
      "2019-05-10 11:25:40,628 : INFO : PROGRESS: at sentence #150000, processed 3711835 words, keeping 5080 word types\n",
      "2019-05-10 11:25:40,696 : INFO : PROGRESS: at sentence #160000, processed 3957792 words, keeping 5139 word types\n",
      "2019-05-10 11:25:40,765 : INFO : PROGRESS: at sentence #170000, processed 4212253 words, keeping 5194 word types\n",
      "2019-05-10 11:25:40,835 : INFO : PROGRESS: at sentence #180000, processed 4462287 words, keeping 5199 word types\n",
      "2019-05-10 11:25:40,906 : INFO : PROGRESS: at sentence #190000, processed 4708013 words, keeping 5207 word types\n",
      "2019-05-10 11:25:41,001 : INFO : PROGRESS: at sentence #200000, processed 4954503 words, keeping 5217 word types\n",
      "2019-05-10 11:25:41,103 : INFO : PROGRESS: at sentence #210000, processed 5197096 words, keeping 5226 word types\n",
      "2019-05-10 11:25:41,183 : INFO : PROGRESS: at sentence #220000, processed 5440475 words, keeping 5236 word types\n",
      "2019-05-10 11:25:41,263 : INFO : PROGRESS: at sentence #230000, processed 5692465 words, keeping 5250 word types\n",
      "2019-05-10 11:25:41,267 : INFO : collected 5250 word types from a corpus of 5701393 raw words and 230370 sentences\n",
      "2019-05-10 11:25:41,268 : INFO : Loading a fresh vocabulary\n",
      "2019-05-10 11:25:41,278 : INFO : effective_min_count=5 retains 4113 unique words (78% of original 5250, drops 1137)\n",
      "2019-05-10 11:25:41,279 : INFO : effective_min_count=5 leaves 5698985 word corpus (99% of original 5701393, drops 2408)\n",
      "2019-05-10 11:25:41,296 : INFO : deleting the raw counts dictionary of 5250 items\n",
      "2019-05-10 11:25:41,298 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2019-05-10 11:25:41,299 : INFO : downsampling leaves estimated 4976480 word corpus (87.3% of prior 5698985)\n",
      "2019-05-10 11:25:41,309 : INFO : estimated required memory for 4113 words and 250 dimensions: 10282500 bytes\n",
      "2019-05-10 11:25:41,309 : INFO : resetting layer weights\n",
      "2019-05-10 11:25:41,373 : INFO : training model with 3 workers on 4113 vocabulary and 250 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-10 11:25:41,376 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:25:42,387 : INFO : EPOCH 1 - PROGRESS: at 18.27% examples, 928699 words/s, in_qsize 4, out_qsize 0\n",
      "2019-05-10 11:25:43,396 : INFO : EPOCH 1 - PROGRESS: at 38.95% examples, 967526 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:25:44,408 : INFO : EPOCH 1 - PROGRESS: at 61.78% examples, 1016559 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:25:45,413 : INFO : EPOCH 1 - PROGRESS: at 82.94% examples, 1023795 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:25:46,124 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:25:46,133 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:25:46,140 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:25:46,141 : INFO : EPOCH - 1 : training on 5701393 raw words (4976227 effective words) took 4.8s, 1044468 effective words/s\n",
      "2019-05-10 11:25:46,144 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:25:47,147 : INFO : EPOCH 2 - PROGRESS: at 21.77% examples, 1110593 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:25:48,156 : INFO : EPOCH 2 - PROGRESS: at 44.78% examples, 1116150 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:25:49,159 : INFO : EPOCH 2 - PROGRESS: at 68.53% examples, 1131165 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:25:50,160 : INFO : EPOCH 2 - PROGRESS: at 90.55% examples, 1122919 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:25:50,555 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:25:50,562 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:25:50,567 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:25:50,568 : INFO : EPOCH - 2 : training on 5701393 raw words (4977352 effective words) took 4.4s, 1125154 effective words/s\n",
      "2019-05-10 11:25:50,570 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:25:51,578 : INFO : EPOCH 3 - PROGRESS: at 20.52% examples, 1044777 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:25:52,584 : INFO : EPOCH 3 - PROGRESS: at 44.24% examples, 1101816 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:25:53,591 : INFO : EPOCH 3 - PROGRESS: at 66.10% examples, 1088864 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:25:54,598 : INFO : EPOCH 3 - PROGRESS: at 81.36% examples, 1006520 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:25:55,434 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:25:55,453 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:25:55,454 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:25:55,456 : INFO : EPOCH - 3 : training on 5701393 raw words (4976343 effective words) took 4.9s, 1018538 effective words/s\n",
      "2019-05-10 11:25:55,461 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:25:56,462 : INFO : EPOCH 4 - PROGRESS: at 20.18% examples, 1034892 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:25:57,472 : INFO : EPOCH 4 - PROGRESS: at 43.69% examples, 1090067 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:25:58,477 : INFO : EPOCH 4 - PROGRESS: at 67.30% examples, 1110240 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:25:59,478 : INFO : EPOCH 4 - PROGRESS: at 90.19% examples, 1118283 words/s, in_qsize 4, out_qsize 0\n",
      "2019-05-10 11:25:59,890 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:25:59,894 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:25:59,902 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:25:59,902 : INFO : EPOCH - 4 : training on 5701393 raw words (4976559 effective words) took 4.4s, 1120558 effective words/s\n",
      "2019-05-10 11:25:59,905 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:26:00,912 : INFO : EPOCH 5 - PROGRESS: at 21.59% examples, 1098427 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:26:01,919 : INFO : EPOCH 5 - PROGRESS: at 42.97% examples, 1070747 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:26:02,921 : INFO : EPOCH 5 - PROGRESS: at 65.36% examples, 1079535 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 11:26:03,925 : INFO : EPOCH 5 - PROGRESS: at 87.51% examples, 1084350 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:26:04,470 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:26:04,480 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:26:04,483 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:26:04,484 : INFO : EPOCH - 5 : training on 5701393 raw words (4976046 effective words) took 4.6s, 1086871 effective words/s\n",
      "2019-05-10 11:26:04,484 : INFO : training on a 28506965 raw words (24882527 effective words) took 23.1s, 1076733 effective words/s\n",
      "2019-05-10 11:26:04,485 : INFO : saving Word2Vec object under word2vec250_char.model, separately None\n",
      "2019-05-10 11:26:04,486 : INFO : not storing attribute vectors_norm\n",
      "2019-05-10 11:26:04,487 : INFO : not storing attribute cum_table\n",
      "2019-05-10 11:26:04,488 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:26:04,551 : INFO : saved word2vec250_char.model\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gensim.models import word2vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sentences = word2vec.LineSentence('./project1_data/corpus_char.txt')\n",
    "w2v_model_char = word2vec.Word2Vec(sentences, sg=0, hs=0, window=5, size=250, min_count=5, workers = 3)\n",
    "w2v_model_char.save(\"word2vec250_char.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(641104,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_x1 = train.title1_tokenized\n",
    "corpus_x2 = train.title2_tokenized\n",
    "corpus = pd.concat([corpus_x1, corpus_x2])\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 11:27:49,226 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-05-10 11:27:49,228 : INFO : collecting all words and their counts\n",
      "2019-05-10 11:27:49,229 : WARNING : Each 'words' should be a list of words (usually unicode strings). First 'words' here is instead plain <class 'str'>.\n",
      "2019-05-10 11:27:49,230 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-05-10 11:27:49,307 : INFO : PROGRESS: at example #10000, processed 375109 words (4950535/s), 2019 word types, 10000 tags\n",
      "2019-05-10 11:27:49,373 : INFO : PROGRESS: at example #20000, processed 747818 words (5701198/s), 2610 word types, 20000 tags\n",
      "2019-05-10 11:27:49,444 : INFO : PROGRESS: at example #30000, processed 1095334 words (4919136/s), 2879 word types, 30000 tags\n",
      "2019-05-10 11:27:49,506 : INFO : PROGRESS: at example #40000, processed 1433188 words (5506742/s), 3109 word types, 40000 tags\n",
      "2019-05-10 11:27:49,568 : INFO : PROGRESS: at example #50000, processed 1765594 words (5438962/s), 3292 word types, 50000 tags\n",
      "2019-05-10 11:27:49,632 : INFO : PROGRESS: at example #60000, processed 2102958 words (5353966/s), 3409 word types, 60000 tags\n",
      "2019-05-10 11:27:49,698 : INFO : PROGRESS: at example #70000, processed 2449397 words (5322621/s), 3499 word types, 70000 tags\n",
      "2019-05-10 11:27:49,758 : INFO : PROGRESS: at example #80000, processed 2777352 words (5567982/s), 3588 word types, 80000 tags\n",
      "2019-05-10 11:27:49,820 : INFO : PROGRESS: at example #90000, processed 3127528 words (5717600/s), 3626 word types, 90000 tags\n",
      "2019-05-10 11:27:49,882 : INFO : PROGRESS: at example #100000, processed 3480124 words (5716264/s), 3669 word types, 100000 tags\n",
      "2019-05-10 11:27:49,945 : INFO : PROGRESS: at example #110000, processed 3816430 words (5375413/s), 3727 word types, 110000 tags\n",
      "2019-05-10 11:27:50,004 : INFO : PROGRESS: at example #120000, processed 4132340 words (5432155/s), 3787 word types, 120000 tags\n",
      "2019-05-10 11:27:50,066 : INFO : PROGRESS: at example #130000, processed 4451092 words (5216691/s), 3847 word types, 130000 tags\n",
      "2019-05-10 11:27:50,129 : INFO : PROGRESS: at example #140000, processed 4799260 words (5561523/s), 3916 word types, 140000 tags\n",
      "2019-05-10 11:27:50,193 : INFO : PROGRESS: at example #150000, processed 5155284 words (5631633/s), 3954 word types, 150000 tags\n",
      "2019-05-10 11:27:50,255 : INFO : PROGRESS: at example #160000, processed 5487135 words (5431988/s), 3997 word types, 160000 tags\n",
      "2019-05-10 11:27:50,316 : INFO : PROGRESS: at example #170000, processed 5806922 words (5333705/s), 4052 word types, 170000 tags\n",
      "2019-05-10 11:27:50,395 : INFO : PROGRESS: at example #180000, processed 6141496 words (4271206/s), 4090 word types, 180000 tags\n",
      "2019-05-10 11:27:50,475 : INFO : PROGRESS: at example #190000, processed 6469348 words (4202991/s), 4112 word types, 190000 tags\n",
      "2019-05-10 11:27:50,552 : INFO : PROGRESS: at example #200000, processed 6821572 words (4638655/s), 4147 word types, 200000 tags\n",
      "2019-05-10 11:27:50,624 : INFO : PROGRESS: at example #210000, processed 7148696 words (4637895/s), 4173 word types, 210000 tags\n",
      "2019-05-10 11:27:50,696 : INFO : PROGRESS: at example #220000, processed 7484107 words (4703612/s), 4205 word types, 220000 tags\n",
      "2019-05-10 11:27:50,767 : INFO : PROGRESS: at example #230000, processed 7832447 words (4940573/s), 4234 word types, 230000 tags\n",
      "2019-05-10 11:27:50,846 : INFO : PROGRESS: at example #240000, processed 8156622 words (4188324/s), 4263 word types, 240000 tags\n",
      "2019-05-10 11:27:50,922 : INFO : PROGRESS: at example #250000, processed 8476670 words (4250168/s), 4287 word types, 250000 tags\n",
      "2019-05-10 11:27:50,992 : INFO : PROGRESS: at example #260000, processed 8800715 words (4734880/s), 4315 word types, 260000 tags\n",
      "2019-05-10 11:27:51,062 : INFO : PROGRESS: at example #270000, processed 9141792 words (4949789/s), 4335 word types, 270000 tags\n",
      "2019-05-10 11:27:51,134 : INFO : PROGRESS: at example #280000, processed 9489492 words (4825404/s), 4351 word types, 280000 tags\n",
      "2019-05-10 11:27:51,202 : INFO : PROGRESS: at example #290000, processed 9809716 words (4768973/s), 4387 word types, 290000 tags\n",
      "2019-05-10 11:27:51,279 : INFO : PROGRESS: at example #300000, processed 10132541 words (4257539/s), 4397 word types, 300000 tags\n",
      "2019-05-10 11:27:51,364 : INFO : PROGRESS: at example #310000, processed 10474599 words (4055210/s), 4434 word types, 310000 tags\n",
      "2019-05-10 11:27:51,432 : INFO : PROGRESS: at example #320000, processed 10815645 words (5102521/s), 4452 word types, 320000 tags\n",
      "2019-05-10 11:27:51,496 : INFO : PROGRESS: at example #330000, processed 11174165 words (5595425/s), 4490 word types, 330000 tags\n",
      "2019-05-10 11:27:51,561 : INFO : PROGRESS: at example #340000, processed 11530040 words (5603622/s), 4522 word types, 340000 tags\n",
      "2019-05-10 11:27:51,623 : INFO : PROGRESS: at example #350000, processed 11874581 words (5562788/s), 4570 word types, 350000 tags\n",
      "2019-05-10 11:27:51,685 : INFO : PROGRESS: at example #360000, processed 12207664 words (5460314/s), 4621 word types, 360000 tags\n",
      "2019-05-10 11:27:51,747 : INFO : PROGRESS: at example #370000, processed 12548868 words (5563571/s), 4657 word types, 370000 tags\n",
      "2019-05-10 11:27:51,817 : INFO : PROGRESS: at example #380000, processed 12896065 words (4986212/s), 4705 word types, 380000 tags\n",
      "2019-05-10 11:27:51,885 : INFO : PROGRESS: at example #390000, processed 13240346 words (5130338/s), 4730 word types, 390000 tags\n",
      "2019-05-10 11:27:51,950 : INFO : PROGRESS: at example #400000, processed 13570908 words (5170947/s), 4751 word types, 400000 tags\n",
      "2019-05-10 11:27:52,031 : INFO : PROGRESS: at example #410000, processed 13918670 words (4322501/s), 4762 word types, 410000 tags\n",
      "2019-05-10 11:27:52,117 : INFO : PROGRESS: at example #420000, processed 14274773 words (4156198/s), 4781 word types, 420000 tags\n",
      "2019-05-10 11:27:52,194 : INFO : PROGRESS: at example #430000, processed 14618947 words (4576101/s), 4801 word types, 430000 tags\n",
      "2019-05-10 11:27:52,268 : INFO : PROGRESS: at example #440000, processed 14948780 words (4464972/s), 4817 word types, 440000 tags\n",
      "2019-05-10 11:27:52,339 : INFO : PROGRESS: at example #450000, processed 15278723 words (4683566/s), 4833 word types, 450000 tags\n",
      "2019-05-10 11:27:52,437 : INFO : PROGRESS: at example #460000, processed 15618973 words (3499504/s), 4848 word types, 460000 tags\n",
      "2019-05-10 11:27:52,525 : INFO : PROGRESS: at example #470000, processed 15965330 words (4011384/s), 4868 word types, 470000 tags\n",
      "2019-05-10 11:27:52,591 : INFO : PROGRESS: at example #480000, processed 16304705 words (5161160/s), 4882 word types, 480000 tags\n",
      "2019-05-10 11:27:52,654 : INFO : PROGRESS: at example #490000, processed 16644041 words (5433079/s), 4887 word types, 490000 tags\n",
      "2019-05-10 11:27:52,720 : INFO : PROGRESS: at example #500000, processed 16988901 words (5314570/s), 4898 word types, 500000 tags\n",
      "2019-05-10 11:27:52,782 : INFO : PROGRESS: at example #510000, processed 17324543 words (5457584/s), 4903 word types, 510000 tags\n",
      "2019-05-10 11:27:52,851 : INFO : PROGRESS: at example #520000, processed 17673122 words (5114252/s), 4915 word types, 520000 tags\n",
      "2019-05-10 11:27:52,913 : INFO : PROGRESS: at example #530000, processed 18014669 words (5577210/s), 4922 word types, 530000 tags\n",
      "2019-05-10 11:27:52,974 : INFO : PROGRESS: at example #540000, processed 18350928 words (5563283/s), 4930 word types, 540000 tags\n",
      "2019-05-10 11:27:53,037 : INFO : PROGRESS: at example #550000, processed 18701781 words (5610699/s), 4937 word types, 550000 tags\n",
      "2019-05-10 11:27:53,100 : INFO : PROGRESS: at example #560000, processed 19043070 words (5528722/s), 4939 word types, 560000 tags\n",
      "2019-05-10 11:27:53,162 : INFO : PROGRESS: at example #570000, processed 19385358 words (5593038/s), 4948 word types, 570000 tags\n",
      "2019-05-10 11:27:53,223 : INFO : PROGRESS: at example #580000, processed 19720164 words (5521915/s), 4958 word types, 580000 tags\n",
      "2019-05-10 11:27:53,285 : INFO : PROGRESS: at example #590000, processed 20064114 words (5655412/s), 4959 word types, 590000 tags\n",
      "2019-05-10 11:27:53,347 : INFO : PROGRESS: at example #600000, processed 20409303 words (5670575/s), 4962 word types, 600000 tags\n",
      "2019-05-10 11:27:53,409 : INFO : PROGRESS: at example #610000, processed 20746905 words (5514889/s), 4968 word types, 610000 tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 11:27:53,470 : INFO : PROGRESS: at example #620000, processed 21080412 words (5593124/s), 4976 word types, 620000 tags\n",
      "2019-05-10 11:27:53,546 : INFO : PROGRESS: at example #630000, processed 21422550 words (4522156/s), 4980 word types, 630000 tags\n",
      "2019-05-10 11:27:53,628 : INFO : PROGRESS: at example #640000, processed 21773433 words (4329830/s), 4987 word types, 640000 tags\n",
      "2019-05-10 11:27:53,638 : INFO : collected 4987 word types and 641104 unique tags from a corpus of 641104 examples and 21812179 words\n",
      "2019-05-10 11:27:53,639 : INFO : Loading a fresh vocabulary\n",
      "2019-05-10 11:27:53,652 : INFO : effective_min_count=1 retains 4987 unique words (100% of original 4987, drops 0)\n",
      "2019-05-10 11:27:53,652 : INFO : effective_min_count=1 leaves 21812179 word corpus (100% of original 21812179, drops 0)\n",
      "2019-05-10 11:27:53,674 : INFO : deleting the raw counts dictionary of 4987 items\n",
      "2019-05-10 11:27:53,675 : INFO : sample=0.001 downsamples 26 most-common words\n",
      "2019-05-10 11:27:53,675 : INFO : downsampling leaves estimated 14017526 word corpus (64.3% of prior 21812179)\n",
      "2019-05-10 11:27:53,688 : INFO : estimated required memory for 4987 words and 250 dimensions: 653571500 bytes\n",
      "2019-05-10 11:27:53,689 : INFO : resetting layer weights\n",
      "2019-05-10 11:28:03,743 : INFO : training model with 4 workers on 4987 vocabulary and 250 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n",
      "2019-05-10 11:28:04,752 : INFO : EPOCH 1 - PROGRESS: at 2.90% examples, 463622 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:05,754 : INFO : EPOCH 1 - PROGRESS: at 5.58% examples, 432388 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:06,778 : INFO : EPOCH 1 - PROGRESS: at 8.63% examples, 431359 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:07,778 : INFO : EPOCH 1 - PROGRESS: at 11.44% examples, 427159 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:08,780 : INFO : EPOCH 1 - PROGRESS: at 14.39% examples, 430089 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:09,798 : INFO : EPOCH 1 - PROGRESS: at 16.93% examples, 419957 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:10,813 : INFO : EPOCH 1 - PROGRESS: at 19.93% examples, 418040 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:11,830 : INFO : EPOCH 1 - PROGRESS: at 22.99% examples, 421318 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:12,870 : INFO : EPOCH 1 - PROGRESS: at 25.27% examples, 409357 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:13,885 : INFO : EPOCH 1 - PROGRESS: at 27.88% examples, 405276 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:14,902 : INFO : EPOCH 1 - PROGRESS: at 31.01% examples, 409169 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:15,902 : INFO : EPOCH 1 - PROGRESS: at 34.28% examples, 414372 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:16,905 : INFO : EPOCH 1 - PROGRESS: at 37.57% examples, 419373 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:17,921 : INFO : EPOCH 1 - PROGRESS: at 40.48% examples, 417856 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:18,922 : INFO : EPOCH 1 - PROGRESS: at 42.97% examples, 414507 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:19,975 : INFO : EPOCH 1 - PROGRESS: at 44.99% examples, 405346 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:21,002 : INFO : EPOCH 1 - PROGRESS: at 47.58% examples, 402451 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:22,011 : INFO : EPOCH 1 - PROGRESS: at 50.55% examples, 404336 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:23,029 : INFO : EPOCH 1 - PROGRESS: at 53.97% examples, 410028 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 11:28:24,040 : INFO : EPOCH 1 - PROGRESS: at 56.89% examples, 410500 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 11:28:25,044 : INFO : EPOCH 1 - PROGRESS: at 58.70% examples, 403708 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:26,065 : INFO : EPOCH 1 - PROGRESS: at 60.64% examples, 398169 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:27,067 : INFO : EPOCH 1 - PROGRESS: at 63.20% examples, 396971 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:28,068 : INFO : EPOCH 1 - PROGRESS: at 65.88% examples, 397346 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:29,093 : INFO : EPOCH 1 - PROGRESS: at 68.98% examples, 399005 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 11:28:30,103 : INFO : EPOCH 1 - PROGRESS: at 72.21% examples, 401416 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:31,105 : INFO : EPOCH 1 - PROGRESS: at 75.04% examples, 401835 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 11:28:32,107 : INFO : EPOCH 1 - PROGRESS: at 77.81% examples, 402072 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:33,116 : INFO : EPOCH 1 - PROGRESS: at 80.26% examples, 400341 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:34,126 : INFO : EPOCH 1 - PROGRESS: at 82.91% examples, 399973 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-10 11:28:35,144 : INFO : EPOCH 1 - PROGRESS: at 86.18% examples, 402490 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:36,157 : INFO : EPOCH 1 - PROGRESS: at 88.32% examples, 399561 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:37,166 : INFO : EPOCH 1 - PROGRESS: at 90.36% examples, 396324 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:38,172 : INFO : EPOCH 1 - PROGRESS: at 93.66% examples, 398902 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:39,177 : INFO : EPOCH 1 - PROGRESS: at 96.72% examples, 400017 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:40,135 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-10 11:28:40,155 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:28:40,164 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:28:40,166 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:28:40,167 : INFO : EPOCH - 1 : training on 21812179 raw words (14659336 effective words) took 36.4s, 402548 effective words/s\n",
      "2019-05-10 11:28:41,223 : INFO : EPOCH 2 - PROGRESS: at 2.32% examples, 354016 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:42,229 : INFO : EPOCH 2 - PROGRESS: at 4.83% examples, 368519 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:43,245 : INFO : EPOCH 2 - PROGRESS: at 7.11% examples, 353426 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:44,253 : INFO : EPOCH 2 - PROGRESS: at 9.88% examples, 365360 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:45,260 : INFO : EPOCH 2 - PROGRESS: at 13.17% examples, 387454 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:46,264 : INFO : EPOCH 2 - PROGRESS: at 16.33% examples, 402509 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:47,264 : INFO : EPOCH 2 - PROGRESS: at 19.77% examples, 413459 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 11:28:48,276 : INFO : EPOCH 2 - PROGRESS: at 23.04% examples, 420853 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:49,283 : INFO : EPOCH 2 - PROGRESS: at 26.38% examples, 426897 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:50,291 : INFO : EPOCH 2 - PROGRESS: at 29.76% examples, 432584 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 11:28:51,323 : INFO : EPOCH 2 - PROGRESS: at 32.39% examples, 426771 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:52,335 : INFO : EPOCH 2 - PROGRESS: at 34.93% examples, 422375 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:53,347 : INFO : EPOCH 2 - PROGRESS: at 37.95% examples, 422774 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:54,360 : INFO : EPOCH 2 - PROGRESS: at 41.33% examples, 426561 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:55,366 : INFO : EPOCH 2 - PROGRESS: at 44.41% examples, 427569 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:56,390 : INFO : EPOCH 2 - PROGRESS: at 46.98% examples, 422702 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:57,391 : INFO : EPOCH 2 - PROGRESS: at 49.24% examples, 417497 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:28:58,393 : INFO : EPOCH 2 - PROGRESS: at 51.94% examples, 416914 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 11:28:59,408 : INFO : EPOCH 2 - PROGRESS: at 54.56% examples, 415541 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:00,421 : INFO : EPOCH 2 - PROGRESS: at 57.16% examples, 413274 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:01,436 : INFO : EPOCH 2 - PROGRESS: at 59.46% examples, 409644 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 11:29:02,455 : INFO : EPOCH 2 - PROGRESS: at 62.32% examples, 409613 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:03,468 : INFO : EPOCH 2 - PROGRESS: at 65.42% examples, 411853 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:04,475 : INFO : EPOCH 2 - PROGRESS: at 68.56% examples, 413556 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:05,492 : INFO : EPOCH 2 - PROGRESS: at 71.40% examples, 413051 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:06,506 : INFO : EPOCH 2 - PROGRESS: at 74.63% examples, 415065 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:07,524 : INFO : EPOCH 2 - PROGRESS: at 77.68% examples, 416062 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:08,541 : INFO : EPOCH 2 - PROGRESS: at 80.42% examples, 415299 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:09,552 : INFO : EPOCH 2 - PROGRESS: at 83.48% examples, 416263 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:10,565 : INFO : EPOCH 2 - PROGRESS: at 86.72% examples, 418340 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:11,576 : INFO : EPOCH 2 - PROGRESS: at 89.97% examples, 419943 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:12,593 : INFO : EPOCH 2 - PROGRESS: at 93.07% examples, 420801 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:13,598 : INFO : EPOCH 2 - PROGRESS: at 96.35% examples, 422293 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:14,599 : INFO : EPOCH 2 - PROGRESS: at 99.45% examples, 423421 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:14,717 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-10 11:29:14,743 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:29:14,749 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:29:14,752 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:29:14,753 : INFO : EPOCH - 2 : training on 21812179 raw words (14657317 effective words) took 34.6s, 423865 effective words/s\n",
      "2019-05-10 11:29:15,780 : INFO : EPOCH 3 - PROGRESS: at 3.25% examples, 505972 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:16,798 : INFO : EPOCH 3 - PROGRESS: at 6.62% examples, 499726 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:17,803 : INFO : EPOCH 3 - PROGRESS: at 9.83% examples, 487503 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:18,810 : INFO : EPOCH 3 - PROGRESS: at 12.75% examples, 471342 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:19,814 : INFO : EPOCH 3 - PROGRESS: at 15.28% examples, 454413 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:20,818 : INFO : EPOCH 3 - PROGRESS: at 18.23% examples, 449190 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:21,822 : INFO : EPOCH 3 - PROGRESS: at 21.52% examples, 451047 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:22,890 : INFO : EPOCH 3 - PROGRESS: at 24.30% examples, 442568 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 11:29:23,938 : INFO : EPOCH 3 - PROGRESS: at 27.06% examples, 434193 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:24,970 : INFO : EPOCH 3 - PROGRESS: at 28.90% examples, 416821 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:25,977 : INFO : EPOCH 3 - PROGRESS: at 31.27% examples, 410302 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:26,998 : INFO : EPOCH 3 - PROGRESS: at 33.42% examples, 401017 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:28,008 : INFO : EPOCH 3 - PROGRESS: at 36.38% examples, 403718 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:29,024 : INFO : EPOCH 3 - PROGRESS: at 39.12% examples, 401844 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:30,028 : INFO : EPOCH 3 - PROGRESS: at 41.74% examples, 400271 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:31,067 : INFO : EPOCH 3 - PROGRESS: at 44.59% examples, 400015 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:32,116 : INFO : EPOCH 3 - PROGRESS: at 47.76% examples, 401606 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:33,143 : INFO : EPOCH 3 - PROGRESS: at 50.99% examples, 405276 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:34,146 : INFO : EPOCH 3 - PROGRESS: at 54.10% examples, 408876 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:35,148 : INFO : EPOCH 3 - PROGRESS: at 57.30% examples, 411501 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:36,164 : INFO : EPOCH 3 - PROGRESS: at 60.04% examples, 411060 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 11:29:37,171 : INFO : EPOCH 3 - PROGRESS: at 63.24% examples, 413315 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:38,184 : INFO : EPOCH 3 - PROGRESS: at 66.28% examples, 415070 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:39,194 : INFO : EPOCH 3 - PROGRESS: at 69.49% examples, 416906 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:40,204 : INFO : EPOCH 3 - PROGRESS: at 72.62% examples, 418134 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:41,244 : INFO : EPOCH 3 - PROGRESS: at 75.87% examples, 419609 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:42,252 : INFO : EPOCH 3 - PROGRESS: at 79.07% examples, 421323 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:43,269 : INFO : EPOCH 3 - PROGRESS: at 81.95% examples, 421255 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:44,286 : INFO : EPOCH 3 - PROGRESS: at 85.14% examples, 422701 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:45,303 : INFO : EPOCH 3 - PROGRESS: at 87.82% examples, 421578 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 11:29:46,305 : INFO : EPOCH 3 - PROGRESS: at 90.49% examples, 420486 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:47,332 : INFO : EPOCH 3 - PROGRESS: at 93.66% examples, 421561 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:48,348 : INFO : EPOCH 3 - PROGRESS: at 96.53% examples, 421122 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:49,353 : INFO : EPOCH 3 - PROGRESS: at 99.73% examples, 422624 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:49,382 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-10 11:29:49,413 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:29:49,415 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:29:49,419 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:29:49,419 : INFO : EPOCH - 3 : training on 21812179 raw words (14660019 effective words) took 34.7s, 422967 effective words/s\n",
      "2019-05-10 11:29:50,435 : INFO : EPOCH 4 - PROGRESS: at 3.16% examples, 498055 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:51,450 : INFO : EPOCH 4 - PROGRESS: at 6.11% examples, 467499 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:52,469 : INFO : EPOCH 4 - PROGRESS: at 9.24% examples, 458467 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:53,472 : INFO : EPOCH 4 - PROGRESS: at 12.42% examples, 459907 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:54,487 : INFO : EPOCH 4 - PROGRESS: at 15.02% examples, 445624 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:55,495 : INFO : EPOCH 4 - PROGRESS: at 18.13% examples, 446089 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:56,509 : INFO : EPOCH 4 - PROGRESS: at 21.44% examples, 447759 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:57,517 : INFO : EPOCH 4 - PROGRESS: at 24.77% examples, 452926 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:58,526 : INFO : EPOCH 4 - PROGRESS: at 28.15% examples, 455729 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:29:59,533 : INFO : EPOCH 4 - PROGRESS: at 31.47% examples, 458027 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 11:30:00,547 : INFO : EPOCH 4 - PROGRESS: at 34.40% examples, 454561 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:01,573 : INFO : EPOCH 4 - PROGRESS: at 37.56% examples, 454118 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:02,589 : INFO : EPOCH 4 - PROGRESS: at 40.66% examples, 451927 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:03,628 : INFO : EPOCH 4 - PROGRESS: at 43.65% examples, 449838 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:04,628 : INFO : EPOCH 4 - PROGRESS: at 46.47% examples, 445983 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:05,644 : INFO : EPOCH 4 - PROGRESS: at 49.19% examples, 442855 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:06,650 : INFO : EPOCH 4 - PROGRESS: at 52.03% examples, 441818 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 11:30:07,653 : INFO : EPOCH 4 - PROGRESS: at 55.41% examples, 445214 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:08,687 : INFO : EPOCH 4 - PROGRESS: at 58.96% examples, 448401 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:09,694 : INFO : EPOCH 4 - PROGRESS: at 62.32% examples, 450333 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:10,699 : INFO : EPOCH 4 - PROGRESS: at 65.79% examples, 453579 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:11,699 : INFO : EPOCH 4 - PROGRESS: at 69.21% examples, 455485 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:12,702 : INFO : EPOCH 4 - PROGRESS: at 72.66% examples, 457317 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:13,729 : INFO : EPOCH 4 - PROGRESS: at 76.05% examples, 458323 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:14,740 : INFO : EPOCH 4 - PROGRESS: at 79.53% examples, 460169 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:15,740 : INFO : EPOCH 4 - PROGRESS: at 82.95% examples, 461957 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:16,748 : INFO : EPOCH 4 - PROGRESS: at 86.41% examples, 463683 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:17,758 : INFO : EPOCH 4 - PROGRESS: at 89.88% examples, 465041 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:18,763 : INFO : EPOCH 4 - PROGRESS: at 93.29% examples, 466207 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:19,767 : INFO : EPOCH 4 - PROGRESS: at 96.72% examples, 467048 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:20,680 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-10 11:30:20,705 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:30:20,714 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:30:20,717 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:30:20,718 : INFO : EPOCH - 4 : training on 21812179 raw words (14659391 effective words) took 31.3s, 468456 effective words/s\n",
      "2019-05-10 11:30:21,748 : INFO : EPOCH 5 - PROGRESS: at 3.41% examples, 530187 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:22,762 : INFO : EPOCH 5 - PROGRESS: at 6.86% examples, 515908 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:23,771 : INFO : EPOCH 5 - PROGRESS: at 10.42% examples, 515653 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-10 11:30:24,784 : INFO : EPOCH 5 - PROGRESS: at 13.86% examples, 511547 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:25,785 : INFO : EPOCH 5 - PROGRESS: at 17.21% examples, 509743 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:26,794 : INFO : EPOCH 5 - PROGRESS: at 20.80% examples, 507219 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:27,810 : INFO : EPOCH 5 - PROGRESS: at 24.30% examples, 507795 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:28,815 : INFO : EPOCH 5 - PROGRESS: at 27.88% examples, 507531 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:29,824 : INFO : EPOCH 5 - PROGRESS: at 31.33% examples, 506465 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:30,825 : INFO : EPOCH 5 - PROGRESS: at 34.79% examples, 506479 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:31,850 : INFO : EPOCH 5 - PROGRESS: at 38.18% examples, 503492 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-10 11:30:32,853 : INFO : EPOCH 5 - PROGRESS: at 41.28% examples, 498364 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:33,884 : INFO : EPOCH 5 - PROGRESS: at 43.64% examples, 485407 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:34,889 : INFO : EPOCH 5 - PROGRESS: at 45.73% examples, 471392 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:35,901 : INFO : EPOCH 5 - PROGRESS: at 48.59% examples, 467453 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:36,903 : INFO : EPOCH 5 - PROGRESS: at 51.33% examples, 463747 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:37,933 : INFO : EPOCH 5 - PROGRESS: at 54.52% examples, 464099 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:38,947 : INFO : EPOCH 5 - PROGRESS: at 57.15% examples, 459247 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:39,951 : INFO : EPOCH 5 - PROGRESS: at 59.69% examples, 454794 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:40,966 : INFO : EPOCH 5 - PROGRESS: at 62.03% examples, 448949 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:41,968 : INFO : EPOCH 5 - PROGRESS: at 64.36% examples, 444137 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:42,971 : INFO : EPOCH 5 - PROGRESS: at 67.00% examples, 441866 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:43,972 : INFO : EPOCH 5 - PROGRESS: at 69.68% examples, 439316 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:44,977 : INFO : EPOCH 5 - PROGRESS: at 72.75% examples, 439464 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:45,980 : INFO : EPOCH 5 - PROGRESS: at 76.09% examples, 441320 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:46,986 : INFO : EPOCH 5 - PROGRESS: at 79.57% examples, 443829 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:47,986 : INFO : EPOCH 5 - PROGRESS: at 83.00% examples, 446134 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:48,989 : INFO : EPOCH 5 - PROGRESS: at 86.37% examples, 447960 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:49,998 : INFO : EPOCH 5 - PROGRESS: at 89.79% examples, 449593 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:51,015 : INFO : EPOCH 5 - PROGRESS: at 93.29% examples, 451483 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:52,034 : INFO : EPOCH 5 - PROGRESS: at 96.81% examples, 452993 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-10 11:30:52,903 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-10 11:30:52,936 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:30:52,940 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:30:52,942 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:30:52,943 : INFO : EPOCH - 5 : training on 21812179 raw words (14658013 effective words) took 32.2s, 454940 effective words/s\n",
      "2019-05-10 11:30:52,944 : INFO : training on a 109060895 raw words (73294076 effective words) took 169.2s, 433195 effective words/s\n"
     ]
    }
   ],
   "source": [
    "d2v_model = Doc2Vec(documents, vector_size=250, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 11:30:52,950 : INFO : saving Doc2Vec object under doc2vec_model, separately None\n",
      "2019-05-10 11:30:52,952 : INFO : storing np array 'vectors_docs' to doc2vec_model.docvecs.vectors_docs.npy\n",
      "2019-05-10 11:30:54,566 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:30:54,665 : INFO : saved doc2vec_model\n"
     ]
    }
   ],
   "source": [
    "d2v_model.save(\"doc2vec_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastText (word-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 11:38:11,088 : INFO : resetting layer weights\n",
      "2019-05-10 11:38:25,294 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-05-10 11:38:41,151 : INFO : collecting all words and their counts\n",
      "2019-05-10 11:38:41,162 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:38:43,585 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-10 11:38:44,994 : INFO : collected 298614 word types from a corpus of 7483099 raw words and 749 sentences\n",
      "2019-05-10 11:38:44,997 : INFO : Loading a fresh vocabulary\n",
      "2019-05-10 11:38:45,200 : INFO : effective_min_count=5 retains 63882 unique words (21% of original 298614, drops 234732)\n",
      "2019-05-10 11:38:45,201 : INFO : effective_min_count=5 leaves 7150609 word corpus (95% of original 7483099, drops 332490)\n",
      "2019-05-10 11:38:45,381 : INFO : deleting the raw counts dictionary of 298614 items\n",
      "2019-05-10 11:38:45,414 : INFO : sample=0.001 downsamples 27 most-common words\n",
      "2019-05-10 11:38:45,415 : INFO : downsampling leaves estimated 6665665 word corpus (93.2% of prior 7150609)\n",
      "2019-05-10 11:38:45,863 : INFO : estimated required memory for 63882 words, 296687 buckets and 250 dimensions: 462677256 bytes\n",
      "2019-05-10 11:38:45,879 : INFO : resetting layer weights\n",
      "2019-05-10 11:38:58,354 : INFO : training model with 3 workers on 63882 vocabulary and 250 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2019-05-10 11:38:58,403 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:38:59,428 : INFO : EPOCH 1 - PROGRESS: at 2.54% examples, 172168 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:00,454 : INFO : EPOCH 1 - PROGRESS: at 12.02% examples, 403323 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:01,461 : INFO : EPOCH 1 - PROGRESS: at 21.23% examples, 476549 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:02,474 : INFO : EPOCH 1 - PROGRESS: at 30.17% examples, 508419 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:03,478 : INFO : EPOCH 1 - PROGRESS: at 38.85% examples, 524437 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:04,483 : INFO : EPOCH 1 - PROGRESS: at 47.40% examples, 534107 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:05,492 : INFO : EPOCH 1 - PROGRESS: at 55.54% examples, 534282 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:06,513 : INFO : EPOCH 1 - PROGRESS: at 63.15% examples, 527855 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:07,525 : INFO : EPOCH 1 - PROGRESS: at 70.36% examples, 520566 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:08,534 : INFO : EPOCH 1 - PROGRESS: at 78.10% examples, 518364 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:09,538 : INFO : EPOCH 1 - PROGRESS: at 85.98% examples, 517567 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-10 11:39:10,541 : INFO : EPOCH 1 - PROGRESS: at 93.46% examples, 514855 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:11,427 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:39:11,437 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:39:11,460 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:39:11,461 : INFO : EPOCH - 1 : training on 7483099 raw words (6665585 effective words) took 13.1s, 510574 effective words/s\n",
      "2019-05-10 11:39:11,465 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:39:12,489 : INFO : EPOCH 2 - PROGRESS: at 2.54% examples, 171773 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:13,520 : INFO : EPOCH 2 - PROGRESS: at 11.21% examples, 375915 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:14,528 : INFO : EPOCH 2 - PROGRESS: at 19.76% examples, 442928 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:15,532 : INFO : EPOCH 2 - PROGRESS: at 28.17% examples, 475220 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:16,556 : INFO : EPOCH 2 - PROGRESS: at 36.98% examples, 497634 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:17,559 : INFO : EPOCH 2 - PROGRESS: at 46.06% examples, 517660 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:18,559 : INFO : EPOCH 2 - PROGRESS: at 52.20% examples, 503091 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:19,591 : INFO : EPOCH 2 - PROGRESS: at 58.61% examples, 490382 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-10 11:39:20,603 : INFO : EPOCH 2 - PROGRESS: at 66.76% examples, 494052 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-10 11:39:21,609 : INFO : EPOCH 2 - PROGRESS: at 73.70% examples, 489300 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-10 11:39:22,639 : INFO : EPOCH 2 - PROGRESS: at 80.64% examples, 484583 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-10 11:39:23,645 : INFO : EPOCH 2 - PROGRESS: at 87.98% examples, 483617 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:24,655 : INFO : EPOCH 2 - PROGRESS: at 94.26% examples, 477576 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-10 11:39:25,657 : INFO : EPOCH 2 - PROGRESS: at 99.07% examples, 465837 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:25,767 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:39:25,780 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:39:25,792 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:39:25,793 : INFO : EPOCH - 2 : training on 7483099 raw words (6665659 effective words) took 14.3s, 465251 effective words/s\n",
      "2019-05-10 11:39:25,795 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:39:26,804 : INFO : EPOCH 3 - PROGRESS: at 2.14% examples, 146723 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-10 11:39:27,810 : INFO : EPOCH 3 - PROGRESS: at 10.81% examples, 369681 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:28,826 : INFO : EPOCH 3 - PROGRESS: at 20.16% examples, 456531 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:29,847 : INFO : EPOCH 3 - PROGRESS: at 29.64% examples, 501680 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:30,863 : INFO : EPOCH 3 - PROGRESS: at 38.45% examples, 519630 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-10 11:39:31,879 : INFO : EPOCH 3 - PROGRESS: at 44.99% examples, 506298 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:32,881 : INFO : EPOCH 3 - PROGRESS: at 51.54% examples, 497499 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:33,890 : INFO : EPOCH 3 - PROGRESS: at 57.28% examples, 481657 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:34,892 : INFO : EPOCH 3 - PROGRESS: at 63.55% examples, 473288 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:35,919 : INFO : EPOCH 3 - PROGRESS: at 69.03% examples, 460400 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-10 11:39:36,921 : INFO : EPOCH 3 - PROGRESS: at 75.57% examples, 457018 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:37,936 : INFO : EPOCH 3 - PROGRESS: at 81.98% examples, 453091 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-10 11:39:38,948 : INFO : EPOCH 3 - PROGRESS: at 89.19% examples, 453792 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-10 11:39:39,969 : INFO : EPOCH 3 - PROGRESS: at 96.40% examples, 454116 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-10 11:39:40,600 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:39:40,603 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:39:40,628 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:39:40,628 : INFO : EPOCH - 3 : training on 7483099 raw words (6665601 effective words) took 14.8s, 449402 effective words/s\n",
      "2019-05-10 11:39:40,631 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:39:41,636 : INFO : EPOCH 4 - PROGRESS: at 2.14% examples, 147030 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:42,662 : INFO : EPOCH 4 - PROGRESS: at 11.08% examples, 375405 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:43,699 : INFO : EPOCH 4 - PROGRESS: at 20.16% examples, 450834 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:44,700 : INFO : EPOCH 4 - PROGRESS: at 27.90% examples, 470342 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:45,708 : INFO : EPOCH 4 - PROGRESS: at 36.18% examples, 488128 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:46,730 : INFO : EPOCH 4 - PROGRESS: at 44.46% examples, 499035 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 11:39:47,743 : INFO : EPOCH 4 - PROGRESS: at 52.34% examples, 503048 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:48,751 : INFO : EPOCH 4 - PROGRESS: at 60.61% examples, 506788 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:49,753 : INFO : EPOCH 4 - PROGRESS: at 68.22% examples, 505313 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:50,765 : INFO : EPOCH 4 - PROGRESS: at 75.17% examples, 499137 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-10 11:39:51,775 : INFO : EPOCH 4 - PROGRESS: at 82.64% examples, 497527 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:52,789 : INFO : EPOCH 4 - PROGRESS: at 90.92% examples, 500283 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:53,794 : INFO : EPOCH 4 - PROGRESS: at 97.86% examples, 496254 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:54,196 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:39:54,210 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:39:54,217 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:39:54,218 : INFO : EPOCH - 4 : training on 7483099 raw words (6666230 effective words) took 13.6s, 490657 effective words/s\n",
      "2019-05-10 11:39:54,221 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:39:55,238 : INFO : EPOCH 5 - PROGRESS: at 2.14% examples, 145390 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:56,251 : INFO : EPOCH 5 - PROGRESS: at 10.01% examples, 339691 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:57,271 : INFO : EPOCH 5 - PROGRESS: at 18.29% examples, 411818 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:58,283 : INFO : EPOCH 5 - PROGRESS: at 26.97% examples, 455231 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:39:59,295 : INFO : EPOCH 5 - PROGRESS: at 35.51% examples, 479453 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:40:00,306 : INFO : EPOCH 5 - PROGRESS: at 43.79% examples, 492895 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:40:01,330 : INFO : EPOCH 5 - PROGRESS: at 52.20% examples, 502075 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-10 11:40:02,339 : INFO : EPOCH 5 - PROGRESS: at 60.35% examples, 504819 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:40:03,341 : INFO : EPOCH 5 - PROGRESS: at 68.22% examples, 505431 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:40:04,347 : INFO : EPOCH 5 - PROGRESS: at 76.50% examples, 508147 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:40:05,364 : INFO : EPOCH 5 - PROGRESS: at 84.65% examples, 509266 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-10 11:40:06,384 : INFO : EPOCH 5 - PROGRESS: at 92.12% examples, 506505 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:40:07,395 : INFO : EPOCH 5 - PROGRESS: at 99.60% examples, 504515 words/s, in_qsize 3, out_qsize 0\n",
      "2019-05-10 11:40:07,408 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:40:07,410 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:40:07,420 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:40:07,420 : INFO : EPOCH - 5 : training on 7483099 raw words (6666157 effective words) took 13.2s, 505070 effective words/s\n",
      "2019-05-10 11:40:07,421 : INFO : training on a 37415495 raw words (33329232 effective words) took 69.1s, 482606 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "sentences = word2vec.LineSentence('./project1_data/corpus.txt')\n",
    "fasttext_model = FastText(sentences, size=250, window=3, min_count=5, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 11:40:08,817 : INFO : saving FastText object under fasttext_model250_word, separately None\n",
      "2019-05-10 11:40:08,830 : INFO : storing np array 'vectors' to fasttext_model250_word.wv.vectors.npy\n",
      "2019-05-10 11:40:09,046 : INFO : storing np array 'vectors_vocab' to fasttext_model250_word.wv.vectors_vocab.npy\n",
      "2019-05-10 11:40:09,242 : INFO : storing np array 'vectors_ngrams' to fasttext_model250_word.wv.vectors_ngrams.npy\n",
      "2019-05-10 11:40:13,151 : INFO : not storing attribute vectors_norm\n",
      "2019-05-10 11:40:13,152 : INFO : not storing attribute vectors_vocab_norm\n",
      "2019-05-10 11:40:13,152 : INFO : not storing attribute vectors_ngrams_norm\n",
      "2019-05-10 11:40:13,153 : INFO : not storing attribute buckets_word\n",
      "2019-05-10 11:40:13,154 : INFO : storing np array 'syn1neg' to fasttext_model250_word.trainables.syn1neg.npy\n",
      "2019-05-10 11:40:13,358 : INFO : storing np array 'vectors_vocab_lockf' to fasttext_model250_word.trainables.vectors_vocab_lockf.npy\n",
      "2019-05-10 11:40:13,559 : INFO : storing np array 'vectors_ngrams_lockf' to fasttext_model250_word.trainables.vectors_ngrams_lockf.npy\n",
      "2019-05-10 11:40:17,272 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:40:17,388 : INFO : saved fasttext_model250_word\n"
     ]
    }
   ],
   "source": [
    "fasttext_model.save(\"fasttext_model250_word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastText (char-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 11:40:17,400 : INFO : resetting layer weights\n",
      "2019-05-10 11:40:32,100 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-05-10 11:40:47,002 : INFO : collecting all words and their counts\n",
      "2019-05-10 11:40:47,005 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:40:47,007 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-10 11:40:47,092 : INFO : PROGRESS: at sentence #10000, processed 268913 words, keeping 3122 word types\n",
      "2019-05-10 11:40:47,164 : INFO : PROGRESS: at sentence #20000, processed 527364 words, keeping 3764 word types\n",
      "2019-05-10 11:40:47,233 : INFO : PROGRESS: at sentence #30000, processed 776712 words, keeping 4033 word types\n",
      "2019-05-10 11:40:47,301 : INFO : PROGRESS: at sentence #40000, processed 1026225 words, keeping 4173 word types\n",
      "2019-05-10 11:40:47,369 : INFO : PROGRESS: at sentence #50000, processed 1274541 words, keeping 4291 word types\n",
      "2019-05-10 11:40:47,434 : INFO : PROGRESS: at sentence #60000, processed 1509680 words, keeping 4441 word types\n",
      "2019-05-10 11:40:47,504 : INFO : PROGRESS: at sentence #70000, processed 1758381 words, keeping 4532 word types\n",
      "2019-05-10 11:40:47,573 : INFO : PROGRESS: at sentence #80000, processed 2002014 words, keeping 4639 word types\n",
      "2019-05-10 11:40:47,639 : INFO : PROGRESS: at sentence #90000, processed 2243756 words, keeping 4717 word types\n",
      "2019-05-10 11:40:47,707 : INFO : PROGRESS: at sentence #100000, processed 2490941 words, keeping 4807 word types\n",
      "2019-05-10 11:40:47,774 : INFO : PROGRESS: at sentence #110000, processed 2731969 words, keeping 4863 word types\n",
      "2019-05-10 11:40:47,842 : INFO : PROGRESS: at sentence #120000, processed 2977005 words, keeping 4912 word types\n",
      "2019-05-10 11:40:47,909 : INFO : PROGRESS: at sentence #130000, processed 3219043 words, keeping 4977 word types\n",
      "2019-05-10 11:40:47,976 : INFO : PROGRESS: at sentence #140000, processed 3467494 words, keeping 5026 word types\n",
      "2019-05-10 11:40:48,045 : INFO : PROGRESS: at sentence #150000, processed 3711835 words, keeping 5080 word types\n",
      "2019-05-10 11:40:48,114 : INFO : PROGRESS: at sentence #160000, processed 3957792 words, keeping 5139 word types\n",
      "2019-05-10 11:40:48,182 : INFO : PROGRESS: at sentence #170000, processed 4212253 words, keeping 5194 word types\n",
      "2019-05-10 11:40:48,256 : INFO : PROGRESS: at sentence #180000, processed 4462287 words, keeping 5199 word types\n",
      "2019-05-10 11:40:48,327 : INFO : PROGRESS: at sentence #190000, processed 4708013 words, keeping 5207 word types\n",
      "2019-05-10 11:40:48,393 : INFO : PROGRESS: at sentence #200000, processed 4954503 words, keeping 5217 word types\n",
      "2019-05-10 11:40:48,459 : INFO : PROGRESS: at sentence #210000, processed 5197096 words, keeping 5226 word types\n",
      "2019-05-10 11:40:48,526 : INFO : PROGRESS: at sentence #220000, processed 5440475 words, keeping 5236 word types\n",
      "2019-05-10 11:40:48,593 : INFO : PROGRESS: at sentence #230000, processed 5692465 words, keeping 5250 word types\n",
      "2019-05-10 11:40:48,596 : INFO : collected 5250 word types from a corpus of 5701393 raw words and 230370 sentences\n",
      "2019-05-10 11:40:48,599 : INFO : Loading a fresh vocabulary\n",
      "2019-05-10 11:40:48,612 : INFO : effective_min_count=5 retains 4113 unique words (78% of original 5250, drops 1137)\n",
      "2019-05-10 11:40:48,613 : INFO : effective_min_count=5 leaves 5698985 word corpus (99% of original 5701393, drops 2408)\n",
      "2019-05-10 11:40:48,628 : INFO : deleting the raw counts dictionary of 5250 items\n",
      "2019-05-10 11:40:48,630 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2019-05-10 11:40:48,632 : INFO : downsampling leaves estimated 4976480 word corpus (87.3% of prior 5698985)\n",
      "2019-05-10 11:40:48,652 : INFO : estimated required memory for 4113 words, 4113 buckets and 250 dimensions: 14625828 bytes\n",
      "2019-05-10 11:40:48,655 : INFO : resetting layer weights\n",
      "2019-05-10 11:40:58,919 : INFO : training model with 3 workers on 4113 vocabulary and 250 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2019-05-10 11:40:58,926 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:40:59,931 : INFO : EPOCH 1 - PROGRESS: at 16.93% examples, 865911 words/s, in_qsize 1, out_qsize 0\n",
      "2019-05-10 11:41:00,936 : INFO : EPOCH 1 - PROGRESS: at 34.65% examples, 867775 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:41:01,940 : INFO : EPOCH 1 - PROGRESS: at 52.77% examples, 874850 words/s, in_qsize 0, out_qsize 0\n",
      "2019-05-10 11:41:02,941 : INFO : EPOCH 1 - PROGRESS: at 70.65% examples, 875530 words/s, in_qsize 3, out_qsize 0\n",
      "2019-05-10 11:41:03,948 : INFO : EPOCH 1 - PROGRESS: at 88.23% examples, 875064 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:41:04,575 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:41:04,590 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:41:04,592 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:41:04,592 : INFO : EPOCH - 1 : training on 5701393 raw words (4976762 effective words) took 5.7s, 878424 effective words/s\n",
      "2019-05-10 11:41:04,595 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:41:05,599 : INFO : EPOCH 2 - PROGRESS: at 16.93% examples, 865774 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:41:06,616 : INFO : EPOCH 2 - PROGRESS: at 35.58% examples, 884262 words/s, in_qsize 1, out_qsize 0\n",
      "2019-05-10 11:41:07,619 : INFO : EPOCH 2 - PROGRESS: at 54.03% examples, 892164 words/s, in_qsize 0, out_qsize 0\n",
      "2019-05-10 11:41:08,621 : INFO : EPOCH 2 - PROGRESS: at 72.17% examples, 892263 words/s, in_qsize 2, out_qsize 0\n",
      "2019-05-10 11:41:09,622 : INFO : EPOCH 2 - PROGRESS: at 89.83% examples, 889874 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:41:10,177 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:41:10,193 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:41:10,197 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:41:10,198 : INFO : EPOCH - 2 : training on 5701393 raw words (4976627 effective words) took 5.6s, 888261 effective words/s\n",
      "2019-05-10 11:41:10,200 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:41:11,213 : INFO : EPOCH 3 - PROGRESS: at 12.20% examples, 626270 words/s, in_qsize 4, out_qsize 1\n",
      "2019-05-10 11:41:12,223 : INFO : EPOCH 3 - PROGRESS: at 25.30% examples, 633284 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:41:13,223 : INFO : EPOCH 3 - PROGRESS: at 43.15% examples, 716420 words/s, in_qsize 1, out_qsize 0\n",
      "2019-05-10 11:41:14,236 : INFO : EPOCH 3 - PROGRESS: at 59.33% examples, 733385 words/s, in_qsize 3, out_qsize 0\n",
      "2019-05-10 11:41:15,241 : INFO : EPOCH 3 - PROGRESS: at 75.18% examples, 743887 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:41:16,252 : INFO : EPOCH 3 - PROGRESS: at 92.86% examples, 763478 words/s, in_qsize 0, out_qsize 0\n",
      "2019-05-10 11:41:16,625 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:41:16,629 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:41:16,639 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:41:16,640 : INFO : EPOCH - 3 : training on 5701393 raw words (4977573 effective words) took 6.4s, 772987 effective words/s\n",
      "2019-05-10 11:41:16,643 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:41:17,649 : INFO : EPOCH 4 - PROGRESS: at 16.28% examples, 831941 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:41:18,650 : INFO : EPOCH 4 - PROGRESS: at 33.40% examples, 838634 words/s, in_qsize 1, out_qsize 0\n",
      "2019-05-10 11:41:19,654 : INFO : EPOCH 4 - PROGRESS: at 51.03% examples, 847461 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:41:20,660 : INFO : EPOCH 4 - PROGRESS: at 68.35% examples, 846842 words/s, in_qsize 1, out_qsize 0\n",
      "2019-05-10 11:41:21,665 : INFO : EPOCH 4 - PROGRESS: at 85.94% examples, 852502 words/s, in_qsize 1, out_qsize 0\n",
      "2019-05-10 11:41:22,507 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:41:22,519 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:41:22,521 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-10 11:41:22,522 : INFO : EPOCH - 4 : training on 5701393 raw words (4976771 effective words) took 5.9s, 846763 effective words/s\n",
      "2019-05-10 11:41:22,524 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:41:23,528 : INFO : EPOCH 5 - PROGRESS: at 16.93% examples, 866943 words/s, in_qsize 5, out_qsize 0\n",
      "2019-05-10 11:41:24,533 : INFO : EPOCH 5 - PROGRESS: at 35.40% examples, 885786 words/s, in_qsize 3, out_qsize 0\n",
      "2019-05-10 11:41:25,536 : INFO : EPOCH 5 - PROGRESS: at 52.59% examples, 872677 words/s, in_qsize 0, out_qsize 0\n",
      "2019-05-10 11:41:26,537 : INFO : EPOCH 5 - PROGRESS: at 68.53% examples, 849782 words/s, in_qsize 1, out_qsize 0\n",
      "2019-05-10 11:41:27,538 : INFO : EPOCH 5 - PROGRESS: at 83.82% examples, 832840 words/s, in_qsize 0, out_qsize 0\n",
      "2019-05-10 11:41:28,479 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-10 11:41:28,487 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-10 11:41:28,498 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-10 11:41:28,499 : INFO : EPOCH - 5 : training on 5701393 raw words (4975834 effective words) took 6.0s, 832862 effective words/s\n",
      "2019-05-10 11:41:28,501 : INFO : training on a 28506965 raw words (24883567 effective words) took 29.6s, 841303 effective words/s\n",
      "2019-05-10 11:41:28,561 : INFO : saving FastText object under fasttext_model250_char, separately None\n",
      "2019-05-10 11:41:28,564 : INFO : storing np array 'vectors_ngrams' to fasttext_model250_char.wv.vectors_ngrams.npy\n",
      "2019-05-10 11:41:32,646 : INFO : not storing attribute vectors_norm\n",
      "2019-05-10 11:41:32,647 : INFO : not storing attribute vectors_vocab_norm\n",
      "2019-05-10 11:41:32,647 : INFO : not storing attribute vectors_ngrams_norm\n",
      "2019-05-10 11:41:32,648 : INFO : not storing attribute buckets_word\n",
      "2019-05-10 11:41:32,650 : INFO : storing np array 'vectors_ngrams_lockf' to fasttext_model250_char.trainables.vectors_ngrams_lockf.npy\n",
      "2019-05-10 11:41:36,572 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-05-10 11:41:36,696 : INFO : saved fasttext_model250_char\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "sentences = word2vec.LineSentence('./project1_data/corpus_char.txt')\n",
    "fasttext_model_char = FastText(sentences, size=250, window=3, min_count=5, workers=3)\n",
    "fasttext_model_char.save(\"fasttext_model250_char\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-as-service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc1 = BertClient()\n",
    "bc2 = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x1_bert_c = []\n",
    "for idx, sent in enumerate(corpus_x1):\n",
    "    train_x1_bert_c.append(sent.replace(' ',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x2_bert_c = []\n",
    "for idx, sent in enumerate(corpus_x2):\n",
    "    train_x2_bert_c.append(sent.replace(' ',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x1_bert = bc1.encode(train_x1_bert_c)\n",
    "train_x2_bert = b2.encode(train_x2_bert_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handcrafted features\n",
    "   - TF-IDF similarity of title 1 and title 2\n",
    "   - Statistics features of rumor keywords\n",
    "   - Overlap ratio of string matching between title 1 and title 2\n",
    "   - Token set ratio matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap ratio of string matching between title 1 and title 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token set ratio matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_ratio_list = []\n",
    "corpus_x1 = np.array(corpus_x1)\n",
    "corpus_x2 = np.array(corpus_x2)\n",
    "\n",
    "for i in range(len(corpus_x1)):\n",
    "    total = len(set(corpus_x1[0].split(' ')+corpus_x2[0].split(' ')))\n",
    "    overlap_ratio_list.append(len(list(set(corpus_x1[i].split(' ')) & set(corpus_x2[i].split(' '))))/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320552"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overlap_ratio_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['overlap_ratio'] = pd.Series(overlap_ratio_list).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0    0.038462\n",
       "3    0.153846\n",
       "1    0.153846\n",
       "2    0.153846\n",
       "9    0.115385\n",
       "Name: overlap_ratio, dtype: float64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['overlap_ratio'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xval,ytrain,yval = train_test_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcf_body = RandomForestClassifier(n_estimators=100,n_jobs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   40.3s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    1.0s finished\n"
     ]
    }
   ],
   "source": [
    "rcf_body.fit(xtrain, ytrain)\n",
    "y_rc_body_pred = rcf_body.predict(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest F1 and Accuracy Scores : \n",
      "\n",
      "F1 score 68.14%\n",
      "Accuracy score 82.53%\n"
     ]
    }
   ],
   "source": [
    "# print metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score , recall_score , precision_score\n",
    "print (\"Random Forest F1 and Accuracy Scores : \\n\")\n",
    "print ( \"F1 score {:.4}%\".format( f1_score(yval, y_rc_body_pred, average='macro')*100 ) )\n",
    "print ( \"Accuracy score {:.4}%\".format(accuracy_score(yval, y_rc_body_pred)*100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "y_rc_body_pred_test = rcf_body.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7978194 , 0.13778322, 0.06439738],\n",
       "       [0.71346107, 0.27265004, 0.01388889],\n",
       "       [0.67297281, 0.26678427, 0.06024292],\n",
       "       ...,\n",
       "       [0.53353043, 0.45266005, 0.01380952],\n",
       "       [0.58165873, 0.39834127, 0.02      ],\n",
       "       [0.55850866, 0.42734848, 0.01414286]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_rc_body_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_label = {v: k for k, v in label_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Category'] = [index_to_label[idx] for idx in np.argmax(y_rc_body_pred_test, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test \\\n",
    "    .loc[:, ['Category']] \\\n",
    "    .reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321187</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321190</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>321189</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>321193</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321191</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   Category\n",
       "0  321187  unrelated\n",
       "1  321190  unrelated\n",
       "2  321189  unrelated\n",
       "3  321193  unrelated\n",
       "4  321191  unrelated"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.columns = ['Id', 'Category']\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77215"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y_rc_body_pred_test).count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2910"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y_rc_body_pred_test).count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y_rc_body_pred_test).count(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rc_body_pred = rcf_body.predict(xval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21450"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y_rc_body_pred).count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1223"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y_rc_body_pred).count(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_body = XGBClassifier(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xval,ytrain,yval = train_test_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240414"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1, verbose=True)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_body.fit(np.array(xtrain), np.array(ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_xgb_body_pred = xgb_body.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost F1 and Accuracy Scores : \n",
      "\n",
      "F1 score 34.91%``\n",
      "Accuracy score 69.63%\n"
     ]
    }
   ],
   "source": [
    "# print metrics  \n",
    "print (\"XGBoost F1 and Accuracy Scores : \\n\")\n",
    "print ( \"F1 score {:.4}%``\".format( f1_score(yval, y_xgb_body_pred, average='macro')*100 ) )\n",
    "print ( \"Accuracy score {:.4}%\".format(accuracy_score(yval, y_xgb_body_pred)*100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_body = LogisticRegression(penalty='l1', verbose=1, n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=3,\n",
       "          penalty='l1', random_state=None, solver='warn', tol=0.0001,\n",
       "          verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "lr_body.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for article section\n",
    "y_body_pred = lr_body.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistig Regression F1 and Accuracy Scores : \n",
      "\n",
      "F1 score 31.56%\n",
      "Accuracy score 68.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# print metrics\n",
    "print (\"Logistig Regression F1 and Accuracy Scores : \\n\")\n",
    "print ( \"F1 score {:.4}%\".format( f1_score(yval, y_body_pred, average='macro')*100 ) )\n",
    "print ( \"Accuracy score {:.4}%\".format(accuracy_score(yval, y_body_pred)*100) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
